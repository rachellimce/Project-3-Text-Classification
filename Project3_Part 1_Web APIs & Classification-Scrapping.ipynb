{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Web APIs & Classification - Part 1 \n",
    "\n",
    "Webscrapping from Reddit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1\">Problem Statement</a></span></li><li><span><a href=\"#Executive-Summary\" data-toc-modified-id=\"Executive-Summary-2\">Executive Summary</a></span></li><li><span><a href=\"#Methodology\" data-toc-modified-id=\"Methodology-3\">Methodology</a></span></li><li><span><a href=\"#Organization-of-Notebook:\" data-toc-modified-id=\"Organization-of-Notebook:-4\">Organization of Notebook:</a></span></li><li><span><a href=\"#Part-1---Webscrapping\" data-toc-modified-id=\"Part-1---Webscrapping-5\">Part 1 - Webscrapping</a></span></li><li><span><a href=\"#Data-Wrangling/Gathering\" data-toc-modified-id=\"Data-Wrangling/Gathering-6\">Data Wrangling/Gathering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-collecting-using-Reddit-API\" data-toc-modified-id=\"Data-collecting-using-Reddit-API-6.1\">Data collecting using Reddit API</a></span></li><li><span><a href=\"#Combining-2-Subreddits-files-into-1\" data-toc-modified-id=\"Combining-2-Subreddits-files-into-1-6.2\">Combining 2 Subreddits files into 1</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "As a member of the Data Science Team in All Wellness online platform, we are tasked to use NLP to reduce the time and efforts required to classify members' online queries into fitness related or diet related, which will then be  channeled to the panel of certified fitness coach or nutritionist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Who we are**\n",
    "\n",
    "With the increasing emphasis on health and wellness, All Wellness is a startup online platform aiming to help people improve their overall well being via different channels. One of our selling point is having a panel of certified fitness and nutrition coaches giving advices to our platform members on the fitness and dietary queries they have in their workout routine or nutrition and diet. \n",
    "\n",
    "**The pain**\n",
    "\n",
    "One issue All Wellness faced often is that when members wants to get advice via the platform, they typically do not label the nature of their question or label wrongly when they fill in the query form. To ensure the right queries gets to the right channel, lots of time and effort is required to manually tag the enormous amount of queries on a daily basis.\n",
    "\n",
    "**What can be done**\n",
    "\n",
    "Using Natural Language Processing, this tagging process can be automated as the queries come in, gets channel in the shortest amount of time, the panel of advisors gets the queries in real time and in turn members can be more engaged when the queries are correctly addressed.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "Using training data from scrapping reddits, we have trained a Linear Regression model (with details decribed in the following sections in the Technical notebook) that is able to give us about **85%** accuracy on classifying the 2 categories. \n",
    "\n",
    "Misclassifications can still occur if members have a generalized query (no specific indication on diet or workout),  has mixed queries/description or doesn't give enough details (short question of 2 or 3 words)\n",
    "\n",
    "Model can be fine-tuned when there are more data to train on.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Overall, the model can be deployed to reduce some time and efforts for classifying, while collecting more data to fine-tune the model. Future enhancements can be made though more robust ensembling of models, and with more data collected. \n",
    "\n",
    "**Future Enhancements**\n",
    "\n",
    "With this solution, we can also consider creating a chatbot to provide members with some standard FAQ when the model detects certain words or word structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "In this project, we will use Reddit's API to scrape 2 subreddits \n",
    "\n",
    "-  ```/r/workout```\n",
    "-  ```/r/diet/```\n",
    "\n",
    "for posts and use NLP to train a classifer to identify the type of query. \n",
    "\n",
    "\n",
    "Models use:\n",
    "- Logistic Regression with CountVectorizer, TF-IDF\n",
    "- Naive Bayes with CountVectorizer, TF-IDF\n",
    "- KNearestNeighbours with (KNN) CountVectorizer, TF-IDF\n",
    "- Support Vector Machines (SVM) with CountVectorizer, TF-IDF\n",
    "\n",
    "The model with highest Accuracy and Matthews Correlation Coefficient (MCC) score for on the validation data set will be deployed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organization of Notebook:\n",
    "\n",
    "- Project3_Part 1_Web APIs & Classification-Scrapping - Problem Statement, Executive Summary and Webscrapping from Reddits\n",
    "- Project3_Part 2_Web APIs & Classification-EDA Modelling - EDA, Modelling and Conclusion\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Webscrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "#standard imports\n",
    "import pandas as pd\n",
    "#so that pandas do not truncate the rows\n",
    "pd.set_option('max_columns', 100) \n",
    "#Set datafrome display format\n",
    "pd.options.display.float_format = \"{:,.3f}\".format\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#graph imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#define the style of sns/plt\n",
    "\n",
    "#API imports\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Regex\n",
    "import regex as re\n",
    "\n",
    "#Lemmatizing and Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#Modelling\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#==KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#==== Classification matrix\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score, matthews_corrcoef\n",
    "from sklearn.metrics import plot_roc_curve, auc, roc_auc_score, RocCurveDisplay\n",
    "\n",
    "#=== Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#=== CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#=== SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#word cloud\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling/Gathering\n",
    "<font color = \"blue\"> \n",
    "\n",
    "**Part 1** of the project focuses on **Data wrangling/gathering/acquisition**. This is a very important skill as not all the data you will need will be in clean CSVs or a single table in SQL. There is a good chance that wherever you land you will have to gather some data from some unstructured/semi-structured sources; when possible, requesting information from an API, but often scraping it because they don't have an API (or it's terribly documented).\n",
    "\n",
    "For this project I will be using the reddit API and scrape 2 subreddits:\n",
    "\n",
    "-  ```/r/workout```\n",
    "-  ```/r/diet/```\n",
    "\n",
    "for tagging and processing</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collecting using Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapreddit(url, filename=None, num_of_posts=100):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function for collecting reddit posts from a subreddit using reddit API\n",
    "    takes in arguement:\n",
    "    - url: base url for the subreddit\n",
    "    - filename: the filename to save the posts to\n",
    "    - num_of_posts: the total number of posts to be scraped\n",
    "    \n",
    "    Note: reddit API allows 25 posts per request. \n",
    "    Hence Function will loop num_of_posts/25 times\n",
    "    \"\"\"\n",
    "    \n",
    "    #if filename is not specified, it will default to scrapreddit.csv\n",
    "    if filename==None:\n",
    "        filename='datasets/scrapreddit.csv'\n",
    "        \n",
    "    posts = []\n",
    "    after = None\n",
    "\n",
    "    #reddit default 25 posts per scrape, loop until desired posts\n",
    "    for a in range(int(np.ceil(num_of_posts/25))):\n",
    "    \n",
    "        if after == None: #first run of getting data\n",
    "            current_url = url\n",
    "        else:\n",
    "            #subsequent runs. Must get the after ID so that we know which post to extract from\n",
    "            current_url = url + '?after=' + after \n",
    "        \n",
    "        print(current_url)\n",
    "    \n",
    "        res = requests.get(current_url, headers={'User-agent': 'DSI 16.1'})\n",
    "    \n",
    "        #Didnt managed to get the data\n",
    "        if res.status_code != 200:\n",
    "            print('Status error', res.status_code)\n",
    "            break\n",
    "    \n",
    "        #assign the requested data to a dict\n",
    "        current_dict = res.json()\n",
    "    \n",
    "        current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "        posts.extend(current_posts)\n",
    "        after = current_dict['data']['after']\n",
    "        \n",
    "        # not the first time we run, we need to retrieve the prev dataframe for appending, and \n",
    "        if a > 0:\n",
    "            #return from prev records so that we can extend the data\n",
    "            prev_posts = pd.read_csv(filename)\n",
    "\n",
    "            print('\\nreading existing file: current length', len(prev_posts))\n",
    "            \n",
    "            prev_posts = prev_posts.append(current_posts, ignore_index=True)\n",
    "\n",
    "            print('\\nWriting new data to file: new length', len(prev_posts))\n",
    "            prev_posts.to_csv(filename, index = False)\n",
    "            \n",
    "            #prev_posts = pd.read_csv('boardgames.csv')\n",
    "            #current_df = pd.DataFrame()\n",
    "            \n",
    "        else:\n",
    "            print('new file created:', len(posts))\n",
    "            pd.DataFrame(posts).to_csv(filename, index = False)\n",
    "    \n",
    "        # generate a random sleep duration to look more 'natural', so that it is not continuously getting data\n",
    "        sleep_duration = random.randint(2,6)\n",
    "        print(f'Done with {(a+1)*25} of {num_of_posts} records, pause for : {sleep_duration} sec')\n",
    "        print('=============================')\n",
    "        time.sleep(sleep_duration)\n",
    "    \n",
    "    print('Completed. Total posts scraped: ', len(posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"blue\"> \n",
    "Scrapping ```/r/workout``` subreddit\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.reddit.com/r/workout.json\n",
      "new file created: 27\n",
      "Done with 25 of 1000 records, pause for : 6 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_iietoq\n",
      "\n",
      "reading existing file: current length 27\n",
      "\n",
      "Writing new data to file: new length 52\n",
      "Done with 50 of 1000 records, pause for : 4 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_ii992l\n",
      "\n",
      "reading existing file: current length 52\n",
      "\n",
      "Writing new data to file: new length 77\n",
      "Done with 75 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_igyulv\n",
      "\n",
      "reading existing file: current length 77\n",
      "\n",
      "Writing new data to file: new length 102\n",
      "Done with 100 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_igzm3b\n",
      "\n",
      "reading existing file: current length 102\n",
      "\n",
      "Writing new data to file: new length 127\n",
      "Done with 125 of 1000 records, pause for : 3 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_igewro\n",
      "\n",
      "reading existing file: current length 127\n",
      "\n",
      "Writing new data to file: new length 152\n",
      "Done with 150 of 1000 records, pause for : 6 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_ig2e50\n",
      "\n",
      "reading existing file: current length 152\n",
      "\n",
      "Writing new data to file: new length 177\n",
      "Done with 175 of 1000 records, pause for : 3 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_if52ab\n",
      "\n",
      "reading existing file: current length 177\n",
      "\n",
      "Writing new data to file: new length 202\n",
      "Done with 200 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_ieq7rb\n",
      "\n",
      "reading existing file: current length 202\n",
      "\n",
      "Writing new data to file: new length 227\n",
      "Done with 225 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_ieck18\n",
      "\n",
      "reading existing file: current length 227\n",
      "\n",
      "Writing new data to file: new length 252\n",
      "Done with 250 of 1000 records, pause for : 4 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_idqdv5\n",
      "\n",
      "reading existing file: current length 252\n",
      "\n",
      "Writing new data to file: new length 277\n",
      "Done with 275 of 1000 records, pause for : 6 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_id5o6y\n",
      "\n",
      "reading existing file: current length 277\n",
      "\n",
      "Writing new data to file: new length 302\n",
      "Done with 300 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_icpevo\n",
      "\n",
      "reading existing file: current length 302\n",
      "\n",
      "Writing new data to file: new length 327\n",
      "Done with 325 of 1000 records, pause for : 3 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_iccgpd\n",
      "\n",
      "reading existing file: current length 327\n",
      "\n",
      "Writing new data to file: new length 352\n",
      "Done with 350 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_ibiuoe\n",
      "\n",
      "reading existing file: current length 352\n",
      "\n",
      "Writing new data to file: new length 377\n",
      "Done with 375 of 1000 records, pause for : 4 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_iat49u\n",
      "\n",
      "reading existing file: current length 377\n",
      "\n",
      "Writing new data to file: new length 402\n",
      "Done with 400 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_iaf18v\n",
      "\n",
      "reading existing file: current length 402\n",
      "\n",
      "Writing new data to file: new length 427\n",
      "Done with 425 of 1000 records, pause for : 6 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_i9pyeq\n",
      "\n",
      "reading existing file: current length 427\n",
      "\n",
      "Writing new data to file: new length 452\n",
      "Done with 450 of 1000 records, pause for : 4 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_i98xoq\n",
      "\n",
      "reading existing file: current length 452\n",
      "\n",
      "Writing new data to file: new length 477\n",
      "Done with 475 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_i88lb0\n",
      "\n",
      "reading existing file: current length 477\n",
      "\n",
      "Writing new data to file: new length 502\n",
      "Done with 500 of 1000 records, pause for : 6 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_i7ndoi\n",
      "\n",
      "reading existing file: current length 502\n",
      "\n",
      "Writing new data to file: new length 527\n",
      "Done with 525 of 1000 records, pause for : 6 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_i6lzhr\n",
      "\n",
      "reading existing file: current length 527\n",
      "\n",
      "Writing new data to file: new length 552\n",
      "Done with 550 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_i61j5v\n",
      "\n",
      "reading existing file: current length 552\n",
      "\n",
      "Writing new data to file: new length 577\n",
      "Done with 575 of 1000 records, pause for : 3 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_i50vfc\n",
      "\n",
      "reading existing file: current length 577\n",
      "\n",
      "Writing new data to file: new length 602\n",
      "Done with 600 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_i4ctat\n",
      "\n",
      "reading existing file: current length 602\n",
      "\n",
      "Writing new data to file: new length 627\n",
      "Done with 625 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_i3v24k\n",
      "\n",
      "reading existing file: current length 627\n",
      "\n",
      "Writing new data to file: new length 652\n",
      "Done with 650 of 1000 records, pause for : 3 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_i3egdw\n",
      "\n",
      "reading existing file: current length 652\n",
      "\n",
      "Writing new data to file: new length 677\n",
      "Done with 675 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_i2jlgw\n",
      "\n",
      "reading existing file: current length 677\n",
      "\n",
      "Writing new data to file: new length 702\n",
      "Done with 700 of 1000 records, pause for : 3 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_i1s7ie\n",
      "\n",
      "reading existing file: current length 702\n",
      "\n",
      "Writing new data to file: new length 727\n",
      "Done with 725 of 1000 records, pause for : 6 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_i0mz7l\n",
      "\n",
      "reading existing file: current length 727\n",
      "\n",
      "Writing new data to file: new length 752\n",
      "Done with 750 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_i07u9q\n",
      "\n",
      "reading existing file: current length 752\n",
      "\n",
      "Writing new data to file: new length 777\n",
      "Done with 775 of 1000 records, pause for : 4 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_hzlwva\n",
      "\n",
      "reading existing file: current length 777\n",
      "\n",
      "Writing new data to file: new length 802\n",
      "Done with 800 of 1000 records, pause for : 3 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_hz56as\n",
      "\n",
      "reading existing file: current length 802\n",
      "\n",
      "Writing new data to file: new length 827\n",
      "Done with 825 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_hy85sv\n",
      "\n",
      "reading existing file: current length 827\n",
      "\n",
      "Writing new data to file: new length 852\n",
      "Done with 850 of 1000 records, pause for : 4 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_hxwoas\n",
      "\n",
      "reading existing file: current length 852\n",
      "\n",
      "Writing new data to file: new length 877\n",
      "Done with 875 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_hxl6wh\n",
      "\n",
      "reading existing file: current length 877\n",
      "\n",
      "Writing new data to file: new length 902\n",
      "Done with 900 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_hx7fo6\n",
      "\n",
      "reading existing file: current length 902\n",
      "\n",
      "Writing new data to file: new length 921\n",
      "Done with 925 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json\n",
      "\n",
      "reading existing file: current length 921\n",
      "\n",
      "Writing new data to file: new length 948\n",
      "Done with 950 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_iietoq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "reading existing file: current length 948\n",
      "\n",
      "Writing new data to file: new length 973\n",
      "Done with 975 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/workout.json?after=t3_ii992l\n",
      "\n",
      "reading existing file: current length 973\n",
      "\n",
      "Writing new data to file: new length 998\n",
      "Done with 1000 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "Completed. Total posts scraped:  998\n"
     ]
    }
   ],
   "source": [
    "scrapreddit('http://www.reddit.com/r/workout.json', filename='datasets/workout.csv', num_of_posts=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"blue\"> \n",
    "Scrapping ```/r/diet``` subreddit\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.reddit.com/r/diet.json\n",
      "new file created: 26\n",
      "Done with 25 of 1000 records, pause for : 6 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_ihfxz2\n",
      "\n",
      "reading existing file: current length 26\n",
      "\n",
      "Writing new data to file: new length 51\n",
      "Done with 50 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_igqkcz\n",
      "\n",
      "reading existing file: current length 51\n",
      "\n",
      "Writing new data to file: new length 76\n",
      "Done with 75 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_ifki8m\n",
      "\n",
      "reading existing file: current length 76\n",
      "\n",
      "Writing new data to file: new length 101\n",
      "Done with 100 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_ie0iz2\n",
      "\n",
      "reading existing file: current length 101\n",
      "\n",
      "Writing new data to file: new length 126\n",
      "Done with 125 of 1000 records, pause for : 3 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_icrzdk\n",
      "\n",
      "reading existing file: current length 126\n",
      "\n",
      "Writing new data to file: new length 151\n",
      "Done with 150 of 1000 records, pause for : 4 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_ib6c4q\n",
      "\n",
      "reading existing file: current length 151\n",
      "\n",
      "Writing new data to file: new length 176\n",
      "Done with 175 of 1000 records, pause for : 3 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_i9sbd7\n",
      "\n",
      "reading existing file: current length 176\n",
      "\n",
      "Writing new data to file: new length 201\n",
      "Done with 200 of 1000 records, pause for : 3 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_i7or6j\n",
      "\n",
      "reading existing file: current length 201\n",
      "\n",
      "Writing new data to file: new length 226\n",
      "Done with 225 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_i667aa\n",
      "\n",
      "reading existing file: current length 226\n",
      "\n",
      "Writing new data to file: new length 251\n",
      "Done with 250 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_i48i2v\n",
      "\n",
      "reading existing file: current length 251\n",
      "\n",
      "Writing new data to file: new length 276\n",
      "Done with 275 of 1000 records, pause for : 6 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_i321b8\n",
      "\n",
      "reading existing file: current length 276\n",
      "\n",
      "Writing new data to file: new length 301\n",
      "Done with 300 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_i1uoar\n",
      "\n",
      "reading existing file: current length 301\n",
      "\n",
      "Writing new data to file: new length 326\n",
      "Done with 325 of 1000 records, pause for : 6 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_i0fmnv\n",
      "\n",
      "reading existing file: current length 326\n",
      "\n",
      "Writing new data to file: new length 351\n",
      "Done with 350 of 1000 records, pause for : 3 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_hyi8zx\n",
      "\n",
      "reading existing file: current length 351\n",
      "\n",
      "Writing new data to file: new length 376\n",
      "Done with 375 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_hx7xwb\n",
      "\n",
      "reading existing file: current length 376\n",
      "\n",
      "Writing new data to file: new length 401\n",
      "Done with 400 of 1000 records, pause for : 4 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_hvgu2i\n",
      "\n",
      "reading existing file: current length 401\n",
      "\n",
      "Writing new data to file: new length 426\n",
      "Done with 425 of 1000 records, pause for : 3 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_huh0so\n",
      "\n",
      "reading existing file: current length 426\n",
      "\n",
      "Writing new data to file: new length 451\n",
      "Done with 450 of 1000 records, pause for : 6 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_htchdy\n",
      "\n",
      "reading existing file: current length 451\n",
      "\n",
      "Writing new data to file: new length 476\n",
      "Done with 475 of 1000 records, pause for : 4 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_hs711t\n",
      "\n",
      "reading existing file: current length 476\n",
      "\n",
      "Writing new data to file: new length 501\n",
      "Done with 500 of 1000 records, pause for : 4 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_hqq5i7\n",
      "\n",
      "reading existing file: current length 501\n",
      "\n",
      "Writing new data to file: new length 526\n",
      "Done with 525 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_hpfpgb\n",
      "\n",
      "reading existing file: current length 526\n",
      "\n",
      "Writing new data to file: new length 551\n",
      "Done with 550 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_hnvqek\n",
      "\n",
      "reading existing file: current length 551\n",
      "\n",
      "Writing new data to file: new length 576\n",
      "Done with 575 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_hm77o4\n",
      "\n",
      "reading existing file: current length 576\n",
      "\n",
      "Writing new data to file: new length 601\n",
      "Done with 600 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_hl0aar\n",
      "\n",
      "reading existing file: current length 601\n",
      "\n",
      "Writing new data to file: new length 626\n",
      "Done with 625 of 1000 records, pause for : 3 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_hjyrwm\n",
      "\n",
      "reading existing file: current length 626\n",
      "\n",
      "Writing new data to file: new length 651\n",
      "Done with 650 of 1000 records, pause for : 4 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_hi13f0\n",
      "\n",
      "reading existing file: current length 651\n",
      "\n",
      "Writing new data to file: new length 676\n",
      "Done with 675 of 1000 records, pause for : 4 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_hgo8hc\n",
      "\n",
      "reading existing file: current length 676\n",
      "\n",
      "Writing new data to file: new length 701\n",
      "Done with 700 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_hfff2f\n",
      "\n",
      "reading existing file: current length 701\n",
      "\n",
      "Writing new data to file: new length 726\n",
      "Done with 725 of 1000 records, pause for : 4 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_hdsrlx\n",
      "\n",
      "reading existing file: current length 726\n",
      "\n",
      "Writing new data to file: new length 751\n",
      "Done with 750 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_hc51zv\n",
      "\n",
      "reading existing file: current length 751\n",
      "\n",
      "Writing new data to file: new length 776\n",
      "Done with 775 of 1000 records, pause for : 4 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_hak0ir\n",
      "\n",
      "reading existing file: current length 776\n",
      "\n",
      "Writing new data to file: new length 801\n",
      "Done with 800 of 1000 records, pause for : 3 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_h8cbf5\n",
      "\n",
      "reading existing file: current length 801\n",
      "\n",
      "Writing new data to file: new length 826\n",
      "Done with 825 of 1000 records, pause for : 3 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_h142pj\n",
      "\n",
      "reading existing file: current length 826\n",
      "\n",
      "Writing new data to file: new length 851\n",
      "Done with 850 of 1000 records, pause for : 4 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_gztecx\n",
      "\n",
      "reading existing file: current length 851\n",
      "\n",
      "Writing new data to file: new length 876\n",
      "Done with 875 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_gydsgh\n",
      "\n",
      "reading existing file: current length 876\n",
      "\n",
      "Writing new data to file: new length 901\n",
      "Done with 900 of 1000 records, pause for : 3 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_gwq7ku\n",
      "\n",
      "reading existing file: current length 901\n",
      "\n",
      "Writing new data to file: new length 922\n",
      "Done with 925 of 1000 records, pause for : 5 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json\n",
      "\n",
      "reading existing file: current length 922\n",
      "\n",
      "Writing new data to file: new length 948\n",
      "Done with 950 of 1000 records, pause for : 4 sec\n",
      "=============================\n",
      "http://www.reddit.com/r/diet.json?after=t3_ihfxz2\n",
      "\n",
      "reading existing file: current length 948\n",
      "\n",
      "Writing new data to file: new length 973\n",
      "Done with 975 of 1000 records, pause for : 4 sec\n",
      "=============================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.reddit.com/r/diet.json?after=t3_igqkcz\n",
      "\n",
      "reading existing file: current length 973\n",
      "\n",
      "Writing new data to file: new length 998\n",
      "Done with 1000 of 1000 records, pause for : 2 sec\n",
      "=============================\n",
      "Completed. Total posts scraped:  998\n"
     ]
    }
   ],
   "source": [
    "scrapreddit('http://www.reddit.com/r/diet.json',filename='datasets/diet.csv', num_of_posts=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"blue\"> \n",
    "Managed to scraped \n",
    "- 998 text records for ```/r/workout``` in ```'workout.csv'```\n",
    "- 998 text records for ```/r/diet``` in ```'diet.csv'``` \n",
    "\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining 2 Subreddits files into 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(998, 111)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading in our data files\n",
    "df_workout = pd.read_csv('datasets/workout.csv')\n",
    "df_workout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combining titles and selftext into one columns call posts\n",
    "df_workout['post'] = df_workout['title'].fillna('')+ ' ' + df_workout['selftext'].fillna('')\n",
    "df_selectedworkout = df_workout[['subreddit','post']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit    0\n",
       "post         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for null values\n",
    "df_selectedworkout.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(914, 2)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop duplicates\n",
    "df_selectedworkout = df_selectedworkout.drop_duplicates()\n",
    "df_selectedworkout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(998, 110)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do the same for diet csv\n",
    "df_diet = pd.read_csv('datasets/diet.csv')\n",
    "df_diet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining titles and selftext into one columns call posts\n",
    "df_diet['post'] = df_diet['title'].fillna('')+ ' ' + df_diet['selftext'].fillna('')\n",
    "df_selecteddiet = df_diet[['subreddit','post']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>diet</td>\n",
       "      <td>Post Workout Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>diet</td>\n",
       "      <td>Thought on a veggie only diet Before someone s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>diet</td>\n",
       "      <td>Raising my calorie intake So I’ve been calorie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>diet</td>\n",
       "      <td>Join our Discord server! Hey everyone! In orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>diet</td>\n",
       "      <td>I have a friend that I'm looking to help [ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>diet</td>\n",
       "      <td>Stress related hypothesis... Is there any scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>diet</td>\n",
       "      <td>Your suggestions would really be appreciated H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>diet</td>\n",
       "      <td>Gilbert syndrome and Oatmeal (bilirubin and ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>diet</td>\n",
       "      <td>Losing weight I’m 6’0 tall and weigh about 85k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>diet</td>\n",
       "      <td>Losing weight while breastfeeding I’m Over 6 w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit                                               post\n",
       "178      diet                                 Post Workout Food \n",
       "299      diet  Thought on a veggie only diet Before someone s...\n",
       "888      diet  Raising my calorie intake So I’ve been calorie...\n",
       "922      diet  Join our Discord server! Hey everyone! In orde...\n",
       "923      diet  I have a friend that I'm looking to help [ques...\n",
       "..        ...                                                ...\n",
       "993      diet  Stress related hypothesis... Is there any scie...\n",
       "994      diet  Your suggestions would really be appreciated H...\n",
       "995      diet  Gilbert syndrome and Oatmeal (bilirubin and ma...\n",
       "996      diet  Losing weight I’m 6’0 tall and weigh about 85k...\n",
       "997      diet  Losing weight while breastfeeding I’m Over 6 w...\n",
       "\n",
       "[79 rows x 2 columns]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for duplicates\n",
    "df_selecteddiet[df_selecteddiet.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(919, 2)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop duplicates\n",
    "df_selecteddiet = df_selecteddiet.drop_duplicates()\n",
    "df_selecteddiet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>workout</td>\n",
       "      <td>Do you need to Gain Weight, Lose Weight, or Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>workout</td>\n",
       "      <td>Beginner's Guide to Working Out  As a personal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>workout</td>\n",
       "      <td>By next Saturday, 1 upvote = 5 pushups, will p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>workout</td>\n",
       "      <td>Morning workout in my adidas. What are yall do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>workout</td>\n",
       "      <td>Day 25 of Zac Efron's 12 Week Baywatch Programme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>diet</td>\n",
       "      <td>Am I Actually \"Intermittent Fasting\"? I have b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>diet</td>\n",
       "      <td>50 calories over calorie allowance I went over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>diet</td>\n",
       "      <td>BEST WHEY PROTEIN FOR WEIGHT LOSS (lean) Which...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>diet</td>\n",
       "      <td>Keep getting cravings/hungry in the evening He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>diet</td>\n",
       "      <td>Why is my diet so expensive to maintain? So I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1833 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit                                               post\n",
       "0      workout  Do you need to Gain Weight, Lose Weight, or Ma...\n",
       "1      workout  Beginner's Guide to Working Out  As a personal...\n",
       "2      workout  By next Saturday, 1 upvote = 5 pushups, will p...\n",
       "3      workout  Morning workout in my adidas. What are yall do...\n",
       "4      workout  Day 25 of Zac Efron's 12 Week Baywatch Programme \n",
       "...        ...                                                ...\n",
       "1828      diet  Am I Actually \"Intermittent Fasting\"? I have b...\n",
       "1829      diet  50 calories over calorie allowance I went over...\n",
       "1830      diet  BEST WHEY PROTEIN FOR WEIGHT LOSS (lean) Which...\n",
       "1831      diet  Keep getting cravings/hungry in the evening He...\n",
       "1832      diet  Why is my diet so expensive to maintain? So I ...\n",
       "\n",
       "[1833 rows x 2 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine 2 datasets\n",
    "df_combine = pd.concat([df_selectedworkout,df_selecteddiet], axis=0, ignore_index=True)\n",
    "df_combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the combined dataset for future manipulations\n",
    "df_combine.to_csv('datasets/combine.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"blue\"> \n",
    "Total of 1833 rows for both posts with 2 columns: subreddit and post text\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "398px",
    "left": "25px",
    "top": "32px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
